{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f1eb294-585d-4cd5-b6f5-7527ae399fdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Names.csv \n",
    "* Dodaj kolumnę z wartością czasu wykonania notatnika w formacie epoch\n",
    "* Dodaj kolumnę w której wyliczysz wzrost w stopach (feet)\n",
    "* Odpowiedz na pytanie jakie jest najpopularniesze imię?\n",
    "* Dodaj kolumnę i policz wiek aktorów \n",
    "* Usuń kolumny (bio, death_details)\n",
    "* Zmień nazwy kolumn - dodaj kapitalizaję i usuń _\n",
    "* Posortuj dataframe po imieniu rosnąco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b502e38-7218-4d42-bfbc-d098f67326c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def file_exists(path):\n",
    "  try:\n",
    "    dbutils.fs.ls(path)\n",
    "    return True \n",
    "  except Exception as e:\n",
    "    if 'java.io.FileNotFoundException' in str(e):\n",
    "      return False\n",
    "    else:\n",
    "      raise\n",
    "\n",
    "namesUrl = \"https://raw.githubusercontent.com/cegladanych/azure_bi_data/main/IMDB_movies/names.csv\"\n",
    "filePath2 = \"/FileStore/tables/Files/\"\n",
    "dbutils.fs.mkdirs(filePath2)\n",
    "namesFile = \"names.csv\"\n",
    "tmp = \"file:/tmp/\"\n",
    "dbfsdestination = \"dbfs:/FileStore/tables/Files/\"\n",
    "#NAMES\n",
    "import urllib.request\n",
    "\n",
    "if (file_exists(filePath2 + namesFile) == False):\n",
    "  urllib.request.urlretrieve(namesUrl,\"/tmp/\" + namesFile)\n",
    "  dbutils.fs.mv(tmp + namesFile,dbfsdestination + namesFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "682ee105-0a36-455f-9c09-98dfba93ba21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "############################################################\n",
    "from pyspark.sql.functions import current_timestamp,unix_timestamp,to_date,floor,datediff\n",
    "from pyspark.sql.functions import col,regexp_replace,year,when\n",
    "\n",
    "filePath = \"dbfs:/FileStore/tables/Files/names.csv\"\n",
    "namesDf = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(filePath)\n",
    "#dodanie nowej kolumny \n",
    "namesDf=namesDf.withColumn('current_time',unix_timestamp(current_timestamp()))\n",
    "namesDf.explain()\n",
    "namesDf = namesDf.withColumn('height [ft]', col('height') / 30.48)\n",
    "namesDf.explain()\n",
    "#jakie najpopularniejsze imie \n",
    "names=namesDf.groupBy('name').count().orderBy('count',ascending=False)\n",
    "names.explain()\n",
    "names.show(1) #David Campbell\n",
    "#policzenie roznicy lat\n",
    "namesDf = namesDf.withColumn(\"date_of_birth\",to_date(regexp_replace(col(\"date_of_birth\"), \"\\\\.\", \"-\"), \"dd-MM-yyyy\"))\n",
    "namesDf = namesDf.withColumn(\"date_of_death\",to_date(regexp_replace(col(\"date_of_death\"), \"\\\\.\", \"-\"), \"dd-MM-yyyy\"))\n",
    "#wyciagnięcie roku\n",
    "namesDf = namesDf.withColumn(\"birth\", year(col(\"date_of_birth\")))\n",
    "namesDf = namesDf.withColumn(\"death\", year(col(\"date_of_death\")))\n",
    "namesDf = namesDf.withColumn(\"years\", when(col(\"birth\").isNotNull() & col(\"death\").isNotNull(), col(\"death\") - col(\"birth\")).otherwise(None) )\n",
    "#usunięcie kolumn\n",
    "namesDf=namesDf.drop('bio','death_details')\n",
    "namesDf.explain()\n",
    "#zmiany nazw kolumn \n",
    "columns=[col_names.replace('_',\"\").capitalize() for col_names in namesDf.columns]\n",
    "namesDf=namesDf.toDF(*columns)\n",
    "namesDf.explain()\n",
    "#sortowanie po imieniu rosnąco \n",
    "name=namesDf.orderBy('Name',ascending=True)\n",
    "name.explain()\n",
    "\n",
    "name.schema\n",
    "display(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f86d9d14-5103-4d4a-8df2-cec361db347d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Movies.csv\n",
    "* Dodaj kolumnę z wartością czasu wykonania notatnika w formacie epoch\n",
    "* Dodaj kolumnę która wylicza ile lat upłynęło od publikacji filmu\n",
    "* Dodaj kolumnę która pokaże budżet filmu jako wartość numeryczną, (trzeba usunac znaki walut)\n",
    "* Usuń wiersze z dataframe gdzie wartości są null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56106f76-52ad-4147-bb11-0c057d5f9ade",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def file_exists(path):\n",
    "  try:\n",
    "    dbutils.fs.ls(path)\n",
    "    return True \n",
    "  except Exception as e:\n",
    "    if 'java.io.FileNotFoundException' in str(e):\n",
    "      return False\n",
    "    else:\n",
    "      raise\n",
    "\n",
    "moviesUrl = \"https://raw.githubusercontent.com/cegladanych/azure_bi_data/main/IMDB_movies/movies.csv\"\n",
    "filePath2 = \"/FileStore/tables/Files/\"\n",
    "dbutils.fs.mkdirs(filePath2)\n",
    "moviesFile = \"movies.csv\"\n",
    "tmp = \"file:/tmp/\"\n",
    "dbfsdestination = \"dbfs:/FileStore/tables/Files/\"\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "if (file_exists(filePath2 + moviesFile) == False):\n",
    "  urllib.request.urlretrieve(moviesUrl,\"/tmp/\" + moviesFile)\n",
    "  dbutils.fs.mv(tmp + moviesFile,dbfsdestination + moviesFile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc238ac0-9f22-4685-b2d2-1a074e813334",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "###################################################################################################################################\n",
    "from pyspark.sql.functions import current_timestamp,unix_timestamp,to_date,floor,datediff\n",
    "from pyspark.sql.functions import col,regexp_replace,year,current_date\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "filePath = \"dbfs:/FileStore/tables/Files/movies.csv\"\n",
    "namesDf = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(filePath)\n",
    "\n",
    "#dodanie kolumny \n",
    "namesDf=namesDf.withColumn('current_time',unix_timestamp(current_timestamp()))\n",
    "namesDf.explain()\n",
    "\n",
    "#ile lat upłyneło od publikacji filmu \n",
    "namesDf = namesDf.withColumn(\"date_published\", to_date(namesDf[\"date_published\"], \"dd.MM.yyyy\"))\n",
    "namesDf = namesDf.withColumn(\"years\", year(namesDf[\"date_published\"]))\n",
    "namesDf = namesDf.withColumn(\"years_published\", year(current_date()) - namesDf[\"years\"])\n",
    "\n",
    "#usunięcie wszytsko z wiersza co nie jest liczba \n",
    "namesDf = namesDf.withColumn(\"budget\",regexp_replace(col(\"budget\"), \"[^0-9]\", \"\"))\n",
    "#i zamiana na typ numeryczny \n",
    "namesDf = namesDf.withColumn(\"budget\",col(\"budget\").cast(IntegerType()))\n",
    "namesDf.explain()\n",
    "\n",
    "#usuną wiersze gdzie jest wartość NULL\n",
    "movie=namesDf.na.drop()\n",
    "movie.explain()\n",
    "\n",
    "display(movie)\n",
    "movie.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18d1e437-12d6-41c0-a8f4-d430ae74431f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ratings.csv\n",
    "* Dodaj kolumnę z wartością czasu wykonania notatnika w formacie epoch\n",
    "* Dla każdego z poniższych wyliczeń nie bierz pod uwagę `nulls` \n",
    "* Kto daje lepsze oceny chłopcy czy dziewczyny dla całego setu\n",
    "* Dla jednej z kolumn zmień typ danych do `long` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2c0837f-c069-4b74-a01c-da9a3d7d0ecc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def file_exists(path):\n",
    "  try:\n",
    "    dbutils.fs.ls(path)\n",
    "    return True \n",
    "  except Exception as e:\n",
    "    if 'java.io.FileNotFoundException' in str(e):\n",
    "      return False\n",
    "    else:\n",
    "      raise\n",
    "\n",
    "ratingsUrl = \"https://raw.githubusercontent.com/cegladanych/azure_bi_data/main/IMDB_movies/ratings.csv\"\n",
    "filePath2 = \"/FileStore/tables/Files/\"\n",
    "dbutils.fs.mkdirs(filePath2)\n",
    "ratingsFile = \"ratings.csv\"\n",
    "tmp = \"file:/tmp/\"\n",
    "dbfsdestination = \"dbfs:/FileStore/tables/Files/\"\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "if (file_exists(filePath2 + ratingsFile) == False):\n",
    "  urllib.request.urlretrieve(ratingsUrl,\"/tmp/\" + ratingsFile)\n",
    "  dbutils.fs.mv(tmp + ratingsFile,dbfsdestination + ratingsFile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75c22c14-a54e-4727-b2ad-f8b631f2b541",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp,unix_timestamp,to_date,floor,datediff\n",
    "from pyspark.sql.functions import col,regexp_replace,avg\n",
    "from pyspark.sql.types import IntegerType,LongType\n",
    "\n",
    "filePath = \"dbfs:/FileStore/tables/Files/ratings.csv\"\n",
    "namesDf = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(filePath)\n",
    "\n",
    "#dodanie kolumny \n",
    "namesDf=namesDf.withColumn('current_time',unix_timestamp(current_timestamp()))\n",
    "namesDf.explain()\n",
    "\n",
    "#kto daje lepsze oceny \n",
    "avg_males = namesDf.filter(col(\"males_allages_avg_vote\").isNotNull()).select(avg(col(\"males_allages_avg_vote\")).alias(\"males_avg\"))\n",
    "avg_females = namesDf.filter(col(\"females_allages_avg_vote\").isNotNull()).select(avg(col(\"females_allages_avg_vote\")).alias(\"females_avg\"))\n",
    "avg_males.explain()\n",
    "avg_females.explain()\n",
    "avg_females.show()\n",
    "avg_males.show()#KOBIETY\n",
    "\n",
    "#zamiana na typ long \n",
    "rate= namesDf.filter(col(\"females_45age_avg_vote\").isNotNull()).withColumn(\"females_45age_avg_vote\",col(\"females_45age_avg_vote\").cast(LongType()))\n",
    "rate.explain()\n",
    "\n",
    "display(rate)\n",
    "rate.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a814ee69-e9bd-4937-b38b-7679cc8f85c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#zadanie 2\n",
    "#JOBS DETAIL-umożliwia przeglądanie wszytskich zadań oraz szczegółowe informacje o każdym z nich\n",
    "#STAGES TAB-wyświetla podsumowanie wszytskich etapów zadań(pokazuje status,umożliwia podgląd szczegółów)\n",
    "#STAGE DETAIL-zawiera informacje o całkowitym czasie wykonywania zadań,podsumowanie poziomów lokalizacji,rozmiarze i liczbie rekordów odczytu Shuffle,powiązanych JOB ID,a także wizualizację DAG z ozanczonymi operacjami.\n",
    "#STORAGE TAB-pokazuje informacje o przechowywanych RDD-ach i DataFrame\n",
    "#ENVIRONMENT TAB-prezentuje wartości zmiennych środowiskowych i konfiguracji,takich jak właściwości JVM,ustawienia Spark i parametry systemowe\n",
    "#EXECUTORS-umożliwia monitorowanie stanu i wydajności executorów,pokazując m.in.zużycie zasobów,liczbe wykonanych zadań oraz sposób zarządzania pamięcią i danymi tymczasowymi \n",
    "#SQL TAB-umożliwia analizę zapytań Spark SQL,prezentując ich czas wykonania,powiązane zadania oraz szczegóły planów wykonania,zarówno logicznych,jak i fizycznych\n",
    "#SQL metrics-umożliwiają szczegółową analize wykonania zapytań\n",
    "#Structured Streaming-można śledzić przebieg zapytań strumieniowych w trybie micro-batch,sprawdzać statystyki ich działania oraz analizowac przyczyny ewntualnych błędów\n",
    "#Streaming-pozwala monitorować opóźnienia harmonogrami i czas przetwarzania poszczególnych mikro-batchy\n",
    "#JDBC/ODBC Server-umożliwia monitorowanie sesji i operacji SQL wykonywwanych przez Spark jako rozproszony silnik SQL,prezentując informacje o czasie działania serwera,aktywnych i zakończonych sesjach,szczegółach zapytań,ich stanie oraz planach wykonania.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "006f9de8-fdfb-48f6-a81f-70596606cd29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#zadanie 3 \n",
    "name.explain()\n",
    "name_group=name.groupBy('Children').avg(\"Height\")\n",
    "name_group.explain()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Cwiczenia 3",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
